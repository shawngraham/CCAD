<!doctype html>

<html lang="en" class="h-100">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="generator" content="Hugo 0.83.1" />
  <link rel="stylesheet" href="https://shawngraham.github.io/CCAD/css/bootstrap.min.css">
  
  
  <title>What Happened to 2021-2022? | Computational Creativity and Archaeological Data</title>
  <style>
.container {
  max-width: 800px;
}
#nav a {
  font-weight: bold;
  color: inherit;
}
#nav a.nav-link-active {
  background-color: #212529;
  color: #fff;
}
#nav-border {
  border-bottom: 1px solid #212529;
}
#main {
  margin-top: 1em;
  margin-bottom: 4em;
}
#home-jumbotron {
  background-color: inherit;
}
#footer .container {
  padding: 1em 0;
}
#footer a {
  color: inherit;
  text-decoration: underline;
}
.font-125 {
  font-size: 125%;
}
.tag-btn {
  margin-bottom: 0.3em;
}
pre {
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  padding: 16px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  background-color: transparent;
  border-radius: 0;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 4px;
}
img,
iframe,
embed,
video,
audio {
  max-width: 100%;
}
</style>
</head>
  <body class="d-flex flex-column h-100">
    <div id="nav-border" class="container">
  <nav id="nav" class="nav justify-content-center">
  
  
  
    
    
      
      
      
      
        
      
    
    
    <a class="nav-link " href="/CCAD/"><i data-feather="home"></i> Home</a>
  
    
    
      
      
      
      
        
      
    
    
    <a class="nav-link " href="/CCAD/notebooks/"><i data-feather="book"></i> Notebooks</a>
  
    
    
      
      
      
      
        
      
    
    
    <a class="nav-link " href="/CCAD/post/"><i data-feather="edit"></i> Updates</a>
  
    
    
      
      
      
      
        
      
    
    
    <a class="nav-link " href="/CCAD/tags/"><i data-feather="tag"></i> Tags</a>
  
    
    
      
      
      
      
        
      
    
    
    <a class="nav-link " href="/CCAD/about/"><i data-feather="smile"></i> About</a>
  
    
    
      
      
      
      
        
      
    
    
    <a class="nav-link " href="/CCAD/index.xml"><i data-feather="rss"></i> RSS</a>
  
  </nav>
</div>
    <div class="container">
      <main id="main">
        

<h1>What Happened to 2021-2022?</h1>


<i data-feather="calendar"></i> <time datetime="2022-04-21">Apr 21, 2022</time>

  <br>
  <i data-feather="tag"></i>
  
  
  <a class="btn btn-sm btn-outline-dark tag-btn" href="https://shawngraham.github.io/CCAD/tags/meta">meta</a>
  
  
  <a class="btn btn-sm btn-outline-dark tag-btn" href="https://shawngraham.github.io/CCAD/tags/recap">recap</a>
  
  
  <a class="btn btn-sm btn-outline-dark tag-btn" href="https://shawngraham.github.io/CCAD/tags/image_recognition">image_recognition</a>
  

<br><br>
<h2 id="general-thrust-of-our-group-this-year">General Thrust of Our Group This Year</h2>
<p>Last year, we moved to more of a skunkworks approach, where I provided some overall direction, but allowed the team to follow their nose where&rsquo;er it leads. In general terms, we&rsquo;re taking an approach to using computer vision to archaeological imagery, for both research, outreach, and as a lens through which we deform the past, to bring new elements into view</p>
<h2 id="graduates">Graduates</h2>
<ul>
<li>Completed MA for Jeff Blackadar, History with Data Science. &ldquo;AUTOMATED METHODS TO COLLECT HISTORICAL INFORMATION FROM LIDAR IMAGES AND MAPS.&rdquo;</li>
</ul>
<p>Abstract: &ldquo;In this thesis I use ‘deep learning’, a form of artificial intelligence, to detect features in landscapes and imagery that are of interest to historians. What’s more, I demonstrate reproducible workflows making these methods available to individual historians who do not have a background in machine learning. The first section of the case studies uses a custom trained deep learning model to detect relict charcoal hearths in LiDAR data of the Earth’s surface. The second section of case studies uses commercially available deep learning services to extract geolocated features from historical maps.&rdquo; Open notebook at <a href="http://jeffblackadar.ca/">http://jeffblackadar.ca/</a></p>
<ul>
<li>Completed MA for Jaime Simons, Public History with Digital Humanities. &ldquo;REMIXING THE OTTAWA:  SOUNDS, SENSATIONS, AND STEAMBOAT CONNECTIONS ON THE OTTAWA RIVER, 1823 to 1949&rdquo;</li>
</ul>
<p>Abstract: &ldquo;For over a century, steamboats plied the Ottawa River. They facilitated the colonial settlement of the Ottawa Valley and supported the logging, mining, and railway industries. They were used for tourism and religious colonization schemes, and were both catalysts and aids for the restructuring of the river and the spreading of disease. Settler reliance on steamboats made them an invaluable piece of early industrial infrastructure, turning the Ottawa River into a water-based highway in the service of settler colonialism, resource extraction, and Indigenous dispossession.
This research essay and EP (Extended Play album), titled Remixing the Ottawa, uses sound and performance theory to engage with the history of steamboat imperialism on the Ottawa River. Through six audio tracks (Merging Temporalities, Rising Waters, Hearing Silence, Colonial Connections, ‘Authentic’ Excursions, and Hymns, Dispossession, and Disease), the EP represents and recreates these histories in the present, to re-forefront their forgotten impacts and hidden harms.
The project website can be found at <a href="https://jaimesimns.github.io/remixingtheottawa/%22">https://jaimesimns.github.io/remixingtheottawa/&quot;</a></p>
<h2 id="publications">Publications</h2>
<ul>
<li>Graham, S, and Simons, J. 2021 Listening to Dura Europos: An Experiment in Archaeological Image Sonification, Internet Archaeology 56. <a href="https://doi.org/10.11141/ia.56.8">https://doi.org/10.11141/ia.56.8</a></li>
</ul>
<p>Abstract: &ldquo;We present an experiment in sonifying archival archaeological imagery to make the act of looking at photography strange and weird. The sounds produced will then arrest us and slow us down, and make apparent to us the different ways that archaeological vision is constructed to particular effect/affect. It makes us alive to what is hidden or elided in the image itself; in slowing down, listening/looking/moving at one, we are moved towards enchantment, and engage in a kind of digital hermeneutics that reveals more than what the lens may have captured.&rdquo;</p>
<ul>
<li>Carter, B., Blackadar, J., &amp; Conner, W. (2021). When Computers Dream of Charcoal: Using Deep Learning, Open Tools, and Open Data to Identify Relict Charcoal Hearths in and around State Game Lands in Pennsylvania. Advances in Archaeological Practice, 1-15. <a href="https://www.cambridge.org/core/journals/advances-in-archaeological-practice/article/when-computers-dream-of-charcoal/EE88E84ECC44E369B8FA002D7353FC2F">doi:10.1017/aap.2021.17</a></li>
</ul>
<p>Abstract: &ldquo;This research employs machine learning (Mask Region-Based Convolutional Neural Networks [Mask R-CNN]) and cluster analysis (Density-based spatial clustering of applications with noise [DBSCAN]) to identify more than 20,000 relict charcoal hearths (RCHs) organized in large “fields” within and around State Game Lands (SGLs) in Pennsylvania. This research has two important threads that we hope will advance the archaeological study of landscapes. The first is the significant historical impact of charcoal production, a poorly understood industry of the late eighteenth to early twentieth century, on the historic and present landscape of the United States. Although this research focuses on charcoal production in Pennsylvania, it has broad application for both identifying and contextualizing historical charcoal production throughout the world and for better understanding modern charcoal production. The second thread is the use of open data, open source, and open access tools to conduct this analysis, as well as the open publication of the resultant data. Not only does this research demonstrate the significance of open access tools and data but the open publication of our code as well as our data allow others to replicate our work, to tweak our code and protocols for their own work, and reuse our results.&rdquo;&rdquo;</p>
<ul>
<li>
<p>Blackadar, Jeff, Benjamin Carter, and Weston Conner. “Object Detection Model, Image Data and Results from the ‘When Computers Dream of Charcoal: Using Deep Learning, Open Tools and Open Data to Identify Relict Charcoal Hearths in and Around State Game Lands in Pennsylvania’ Paper.” Journal of Open Archaeology Data 9 (December 27, 2021): 12. <a href="https://doi.org/10.5334/joad.81">https://doi.org/10.5334/joad.81</a>.</p>
</li>
<li>
<p>Blackadar, Jeff, Benjamin Carter, and Weston Conner. “RCH Detection with Mask R-CNN Images.” Zenodo, March 4, 2021. <a href="https://doi.org/10.5281/zenodo.4583945">https://doi.org/10.5281/zenodo.4583945</a>.</p>
</li>
</ul>
<h2 id="presentations">Presentations</h2>
<ul>
<li>Presentation by Scott Coleman on RTI and Numismatics - Underhill Graduate Colloquium, Carleton University</li>
<li>Presentation by Kavita Mistry on Photogrammetry from Archival Photos - Underhill Colloquium, Carleton University</li>
</ul>
<h2 id="experiments-underway">Experiments Underway</h2>
<p>At our intake meeting for this year, new students Noah Chapman, Kavita Mistry, and Scott Coleman decided they wanted to see if they could create creative, immersive, pedagogical experiences from legacy data. They are embarking on building a reproducible workflow for procedurally modeling built spaces from site plans, as they become known through trench notebooks and other ancillary records.</p>
<h2 id="on-our-to-do-list">On Our To-Do List</h2>
<ul>
<li>
<p>eye tracking on archival archaeological photos, like the ones from dura europos (Graham &amp; Simons, 2021). The sonification just took three lines through each image at the same spot to sample data; but what if we sonified the data that people actually looked at? and do people read those old photos differently than they would contemporary archaeological photos?</p>
</li>
<li>
<p>The Imagination of the Machines. This paper is nearing completiong, and features 3d photogrammetric reconstructions of the latent space of computers trained to see like archaeologists.</p>
</li>
</ul>
<h2 id="archaeology-in-space">Archaeology in Space</h2>
<p>How do you record archaeological data in space? On the ground, we understand and create archaeological knowledge by reading the positioning of materials in 3d space; broadly speaking, the depth dimension stands in as a proxy for time. That is to say, <em>gravity</em> is fundamental for creating archaeological knowledge. But when you have no gravity? What then? In december, I was asked for ideas on how to develop a system for the International Space Station Archaeology Project to enable them to record material culture in-situ on the space station - the astronauts take photos, the archaeologists study them. I wrote about the system I cobbled together and which Chantal Brousseau then polished up and made workable on the <a href="https://issarchaeology.org/how-do-you-get-from-an-astronauts-photo-to-usable-archaeological-data/">International Space Station Archaeology Project blog</a>.</p>
<p>I want to repurpose this system to work with other kinds of &lsquo;weird&rsquo; archaeology.  I have access to an orphaned archaeological collection from the Canadian Museum of Nature; in this case, it lacks context, not gravity. But maybe gravity and context are just synonyms? Anyway, I think this tool will be useful in this case and we&rsquo;re planning out an experiment.</p>



      </main>
    </div>
    
<footer id="footer" class="mt-auto text-center text-muted">
  <div class="container">
    Made with <a href="https://gohugo.io/">Hugo</a> &amp; <a href="https://github.com/zwbetz-gh/vanilla-bootstrap-hugo-theme">Vanilla</a>
  </div>
</footer>

    <script src="https://shawngraham.github.io/CCAD/js/feather.min.js"></script>
<script>
  feather.replace()
</script>


    
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-123456789-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  </body>
</html>